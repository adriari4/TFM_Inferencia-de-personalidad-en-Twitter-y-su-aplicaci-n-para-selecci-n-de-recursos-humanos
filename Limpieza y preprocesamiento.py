# -*- coding: utf-8 -*-
"""LIMPIEZA Y PRE-PROCESADO POR ETIQUETAS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12lKiiskswRx0hBtU-RU5UgGufSIIiQP1
"""

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Subir el archivo al entorno de Colab
from google.colab import files
uploaded = files.upload()

# Obtener el nombre del archivo subido
file_name = next(iter(uploaded))

# Descargar recursos necesarios de NLTK
nltk.download('stopwords')
nltk.download('wordnet')

# Cargar el dataset
df = pd.read_csv(file_name)

# Paso 1 Asegurarse de que todos los datos son cadenas de texto
df = df.astype(str)

# Identificar columnas que NO deben ser limpiadas
columns_to_exclude = df.columns[:2]  # Asumiendo que las columnas 1 y 2 son las que quieres excluir

# Aplicar los pasos de limpieza solo a las columnas relevantes
columns_to_process = [col for col in df.columns if col not in columns_to_exclude]

# Paso 2: Eliminación de los hipervínculos
df[columns_to_process] = df[columns_to_process].apply(lambda col: col.map(lambda x: re.sub(r'http\S+|www\.\S+', '', x)))

# Paso 3: Eliminación de signos de puntuación, interrogación o exclamación
df[columns_to_process] = df[columns_to_process].apply(lambda col: col.map(lambda x: re.sub(r'[^\w\s]', '', x)))

# Paso 4: Eliminación de espacios al principio y al final de cada frase
df[columns_to_process] = df[columns_to_process].apply(lambda col: col.map(lambda x: x.strip()))

# Paso 5: Eliminación de conjuntos de letras no reconocidos como palabras
df[columns_to_process] = df[columns_to_process].apply(lambda col: col.map(lambda x: ' '.join([word for word in x.split() if word.isalpha() and len(word) > 2])))

# Paso 6: Conversión de los textos a minúsculas
df[columns_to_process] = df[columns_to_process].apply(lambda col: col.map(lambda x: x.lower()))

# Paso 7: Eliminación de palabras vacías (StopWords)
stop_words = set(stopwords.words('english'))
df[columns_to_process] = df[columns_to_process].apply(lambda col: col.map(lambda x: ' '.join([word for word in x.split() if word not in stop_words])))

# Paso 8: Aplicación de técnicas de lematización
lemmatizer = WordNetLemmatizer()
df[columns_to_process] = df[columns_to_process].apply(lambda col: col.map(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()])))

# Guardar el dataset preprocesado
cleaned_file_name = 'cleaned_' + file_name
df.to_csv(cleaned_file_name, index=False)

# Descargar el archivo preprocesado a tu computadora
files.download(cleaned_file_name)





